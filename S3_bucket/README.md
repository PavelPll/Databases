# OBJECTIVE: Create AWS S3 bucket and use it for ETL Spark project.
## Create AWS S3 bucket
AWS S3 bucket can be created directly using AWS account. Another option, I am using [here](https://github.com/PavelPll/Spark-ETL-ML/blob/main/Scala_ETL/toS3bucket.scala) (or see **toS3bucket.scala**), is the creation of the bucket directly from the code using the class [AmazonS3Client](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Client.html). In order to get the required AWS_ACCESS_KEY and AWS_SECRET_KEY one has to create new IAM (Identity and Access Management) user in AWS account. Choose "Attach policies directly" then "AmazonS3FullAccess" then press "Create user". After user is created go to his "Security Credentials" and press "Create access key" for "Applications running outside AWS". Two libraries aws-java-sdk-bundle and hadoop-aws need to be added (to build.sbt file  and to spark-shell/spark-submit after "--packages"). Check if all hadoop-* libraries have the same version (3.3.1 in my case) to avoid multple ClassNotFoundException etc. 
## Use created AWS S3 bucket for ETL Spark project
[Here (line 80)](https://github.com/PavelPll/Spark-ETL-ML/blob/main/Scala_ETL/ETL.scala) I used the function toS3(spark:SparkSession, df:DataFrame, fileName:String) from **toS3bucket.scala** to save the DataFrame df as fileName to S3 bucket.
